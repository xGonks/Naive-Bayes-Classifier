---
title: "Modeling"
author: "Camila"
format:
   html:
     toc: true
     html-math-method: katex
     embed-resources: true
     self-contained-math: true
     df-print: kable
editor: source
---

# Sparce Matrix Process

```{r}
library(tidyverse)
library(tidytext)
library(Matrix)
```

```{r}
df = read_delim("../data/data_pages.csv", delim = "#$%#$%", trim_ws = TRUE)

```

```{r}
df = df %>%
  filter(!is.na(title), !is.na(description)) #quitar observaciones nan
```

Agregamos una columna de identificador de la historia

```{r}
df = df %>%
  mutate(doc_id = paste0(row_number(), "_", title))
```

Vemos los nombres de las columnas

```{r}
colnames(df)
```

```{r}
tokens = df %>%
  select(doc_id, category, description) %>%  #Elegimos las variables de doc_id , category y description
  unnest_tokens(word, description) #Las palabras de decription se separan individualmente
```

Quitar posibles muletillas o nexos
```{r}
tokens <- tokens %>%
  filter(nchar(word) >= 3)
```


```{r}
data("stop_words")
tokens = tokens %>%
  anti_join(stop_words, by = "word") #Elimina las palabras de tokens comparando la columna word
```


Quitar palabras que casi no aparecen
```{r}
word_freq <- tokens %>%
  count(word, sort = TRUE)

tokens <- tokens %>%
  semi_join(filter(word_freq, n >= 5), by = "word") 
```

filtrar con palabras más frecuentes
```{r}
top_words <- word_freq %>%
  slice_head(n = 2000) %>%        # pick top 20000 words
  pull(word)
```


```{r}
contar_palabras = tokens %>%
  count( category, doc_id, word, sort = TRUE) #Contamos cuantas veces sale cada palabra y ordenamos de mayor a menor
```

```{r}
sparse_matrix = contar_palabras %>% #El conteo de las palabras se convierte en matriz
  tuits_df %>%
  unnest_tokens(input = "text", output = "palabra") %>%
  count(screen_name, status_id, palabra) %>%
  spread(key = palabra, value = n)

sparse_matrix$category <- sparse_matrix$category[match(rownames(sparse_matrix), df$doc_id)]
```

```{r}
categories = df %>%
  distinct(doc_id, category) %>% #Se eligen las categorias
  arrange(match(doc_id, rownames(sparse_matrix))) %>% #Hacemos que coincidan con las filas de la matriz
  pull(category)

head(categories)
```

```{r}
sparse_matrix
```

```{r}
categories
```

# Modelado

## Extraer datos

Vamos a hacer una visualización más intuitiva solo para comprender cómo trabajaremos la matriz

```{r}
library(tibble)

p <- ncol(sparse_matrix)
df_preview <- as_tibble(
  as.matrix(sparse_matrix[1:10, c(1:5, (p-4):p)]), #ver comienzo y final de la matriz
  rownames = "doc_id"
)

df_preview
```

No se unirá categoría para evitar una matriz densa y evitar pasar el límite de memoria de R.

```{r}
stories <- sparse_matrix    
target <- categories 
```

# Limpiar datos

```{r}
glimpse(stories)
```

```{r}
glimpse(target)
```

```{r}
#data = data |> checar
        #filter(!is.na(select(category))); al no ser un data frame lo adaptamos
keep = !is.na(target)

stories = stories[keep, ]
target  = target[keep]
```

# Dividir matriz

```{r}
library(tidymodels)
library(caret)
```

Ponemos una semilla para replicabilidad.

```{r}
set.seed(123)
```

Adaptamos el split a ambas variables

```{r}
#story_split= initial_split(data, prop = 0.7)n <- nrow(stories)
train_idx = sample(seq_len(n), size = floor(0.7 * n)) lo hacemos por índices

train_stories <- stories[train_idx, ]
train_target  <- target[train_idx]

test_stories  <- stories[-train_idx, ]
test_target   <- target[-train_idx]
```

```{r}
story_train = training(story_split)
story_test = testing(story_split)
```

```{r}
install.packages("e1071")
```

```{r}
library(e1071)
```

```{r}
NB_cl1 = naiveBayes(category ~ . , data = diabetes_train)
```

```{r}
y_pred1 = predict(NB_cl1, newdata = story_test)
```

## Matriz confusión

```{r}
confusionMatrix(y_pred1, story_test[["category"]])
```

```{r}
cm = table(story_test$category, y_pred1)
```

```{r}
sum(diag(cm))/ sum(cm)
```

# Naive bayes método 2

```{r}
install.packages("naivebayes")
```

```{r}
library(naivebayes)
```

```{r}
NB_cl2 = naivebayes(category ~ . , data = story_train)

```

```{r}
y_pred2 = predict(NB_cl2, story_test)
```

## Cofusion Matrix

```{r}
confusionMatrix(y_pred2, story_test[["category"]])
```

# Simplificar categorías

REHACER SPLIT!!

```{r}
data <- stories |>
  mutate(
    binary_cat = case_when(
      category == "Haunted Places" ~ "Haunted Places",
      TRUE ~ "Other"   # default for all other cases
    )
  )
```

## Clasificación

```{r}
NB_cl3 = naiveBayes(binary_cat ~ . , data = diabetes_train)
```

```{r}
y_pred3 = predict(NB_cl3, newdata = story_test)
```

```{r}
confusionMatrix(y_pred3, story_test[["binary_cat"]])
```

```{r}
NB_cl4 = naivebayes(binary_cat ~ . , data = story_train)

```

```{r}
y_pred4 = predict(NB_cl4, story_test)
```

```{r}
confusionMatrix(y_pred4, story_test[["binary_cat"]])
```

# Distribución Poisson

```{r}
NB_pois = naive_bayes(binary_cat ~ ., train, usepoisson = TRUE)
```

```{r}
y_predP = predict(NB_pois, story_test)
```

```{r}
confusionMatrix(y_predP, story_test[["binary_cat"]])
```

# Laplace smoothening

```{r}
NB_poisLAP = naive_bayes(binary_cat ~ ., train, usepoisson = TRUE, laplace = 1)
```

```{r}
y_predPLAP = predict(NB_poisLAP, story_test)
```

```{r}
confusionMatrix(y_predPLAP, story_test[["binary_cat"]])
```

# Selección de hiperparámetros

```{r}
library(caret)
```

```{r}
X_df = story_train |> select(-binary_cat)
y   = story_train$binary_cat
```

```{r}
laplace_vals = seq(0.1, 5, by = 0.5)
```

```{r}
outer_folds = createFolds(y, k = 5, returnTrain = FALSE)
inner_folds = 5
```

```{r}
cv_y_test = c()
cv_y_pred = c()
```

```{r}

for (outer in outer_folds) {
  X_train_outer = X_df[-outer, ]
  y_train_outer = y[-outer]
  X_test_outer  = X_df[outer, ]
  y_test_outer  = y[outer]
  
  # Inner loop: elegir mejor Laplace
  best_alpha = NULL
  best_acc = 0
  
  inner_idx = createFolds(y_train_outer, k = inner_folds, returnTrain = FALSE)
  
  for (lap in laplace_vals) {
    acc_inner = c()
    for (fold in inner_idx) {
      X_sub_train = X_train_outer[-fold, ]
      y_sub_train = y_train_outer[-fold]
      X_val       = X_train_outer[fold, ]
      y_val       = y_train_outer[fold]
      
      model_inner = naive_bayes(x = X_sub_train, y = y_sub_train,
                                 laplace = lap, usepoisson = TRUE)
      pred_val = predict(model_inner, X_val)
      acc_inner = c(acc_inner, mean(pred_val == y_val))
    }
    
    avg_acc = mean(acc_inner)
    if (avg_acc > best_acc) {
      best_acc = avg_acc
      best_alpha = lap
    }
  }
```

```{r}
 
  cat("***************\n")
  cat("Best laplace:", best_alpha, 
      " Best Acc (validation):", round(best_acc, 4), "\n")
  
```

```{r}
# Train final model con el mejor laplace encontrado
  final_model = naive_bayes(x = X_train_outer, y = y_train_outer,
                             laplace = best_alpha, usepoisson = TRUE)
  pred_outer = predict(final_model, X_test_outer)
  
  acc_test = mean(pred_outer == y_test_outer)
  cat("Acc (test):", round(acc_test, 4), "\n")
  
  cv_y_test = c(cv_y_test, as.character(y_test_outer))
  cv_y_pred = c(cv_y_pred, as.character(pred_outer))
}
```

```{r}
cm = confusionMatrix(factor(cv_y_pred, levels = levels(y)),
                      factor(cv_y_test, levels = levels(y)))
print(cm)
```
